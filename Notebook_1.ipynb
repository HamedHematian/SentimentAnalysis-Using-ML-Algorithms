{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebook_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "22BTzbyyCP5-",
        "i7WD7DGNCP6L",
        "mNzMUj-8Jxwt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VisrIT1usut-"
      },
      "source": [
        "# Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDjbIIjkCP52",
        "outputId": "9fca508a-3939-4014-a16d-497e8fd5fa16"
      },
      "source": [
        "! pip install contractions\n",
        "! pip install unidecode\n",
        "import unidecode\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "# import sklearn diffrent algorithms\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
        "# other necessary libraries\n",
        "import pickle as pk\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "# this module just prevents warnings to be shown and get output messy\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "# download nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 34.1 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 53.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85453 sha256=70a606402cd3745c0c5985cfeee6e249538fb3b03eb1e5f7789b9d0df362611b\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 29.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.2\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSaP4P8IRSZG",
        "outputId": "8ba29009-bb1b-4f5a-e67d-8f776952bbc7"
      },
      "source": [
        "# download dtaset from drive\n",
        "! gdown --id 1-1qV9uGkvK-RRIF35e5sxnIFQArI1mA8\n",
        "# read dataset\n",
        "dataset = pd.read_csv('dataset.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-1qV9uGkvK-RRIF35e5sxnIFQArI1mA8\n",
            "To: /content/dataset.csv\n",
            "100% 59.7M/59.7M [00:00<00:00, 129MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22BTzbyyCP5-"
      },
      "source": [
        "#  PART A "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTqrfkENrGud"
      },
      "source": [
        "## In this section i define 3 levels of preprocessing then using bag of words vectorization we turn dataset to vectors and apply 3 diffrent algorithms (LR, SVM, KNN) on all 3 levels of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm0GnrGpCP5-"
      },
      "source": [
        "# define class of preprocessing . three methods of this class do 3 diffrent level of preprocessing\n",
        "class Preprocessing:\n",
        "    \n",
        "    def __init__(self,level):\n",
        "        self.level = level\n",
        "        \n",
        "    def level_one_process(self,sent):\n",
        "        return word_tokenize(sent)\n",
        "    \n",
        "    def level_two_process(self,sent):\n",
        "        # delete numbers\n",
        "        sent = re.sub(\"\\d+\", \"\", sent)\n",
        "        # to lowercase\n",
        "        sent = sent.lower()\n",
        "        # tokenization and deleting punctuation marks\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        return tokenizer.tokenize(sent)\n",
        "        \n",
        "    def level_three_process(self,sent):\n",
        "        # remove html\n",
        "        sent = self.remove_html_tags(sent)\n",
        "        # convert accented\n",
        "        sent = self.remove_accented_chars(sent)\n",
        "        # apply level 2 processing first\n",
        "        words = self.level_two_process(sent)\n",
        "        # find and delete stop words\n",
        "        stop_words = stopwords.words('english')\n",
        "        new_words = [w for w in words if not w in stop_words]\n",
        "        # lemmatization\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        new_words = list(map(lemmatizer.lemmatize,new_words))\n",
        "        return new_words\n",
        "        \n",
        "    def preprocess_sentence(self,sent):\n",
        "        if self.level == 1:\n",
        "            processed_sent = self.level_one_process(sent)\n",
        "        elif self.level == 2:\n",
        "            processed_sent = self.level_two_process(sent)\n",
        "        elif self.level == 3:\n",
        "            processed_sent = self.level_three_process(sent)\n",
        "            \n",
        "        return processed_sent\n",
        "\n",
        "    def remove_html_tags(self,sent):\n",
        "      soup = BeautifulSoup(sent, \"html.parser\")\n",
        "      stripped_sent = soup.get_text(separator=\" \")\n",
        "      return stripped_sent\n",
        "\n",
        "    def remove_accented_chars(self,sent):\n",
        "      return unidecode.unidecode(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_2QPCyzCP6B"
      },
      "source": [
        "# a class to generate bag-of-words vectors for words\n",
        "class BagOfWords:\n",
        "    def __init__(self,min_freq):\n",
        "        self.min_freq = min_freq\n",
        "    \n",
        "    def make_vactors(self,text):\n",
        "        CV = CountVectorizer(min_df=self.min_freq)\n",
        "        bow = CV.fit_transform(text)\n",
        "        return bow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmLjnaOsCP6C"
      },
      "source": [
        "# a function to produce results\n",
        "def analysis(labels,predictions):\n",
        "    print(\"Report Classification\\n\",classification_report(labels,predictions,target_names=[\"positive\",\"negative\"]))\n",
        "    print(\"Matrix Confusion\\n\",confusion_matrix(labels,predictions))\n",
        "    print(\"Accuracy\\n\",accuracy_score(labels,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKS80JcrCP6D"
      },
      "source": [
        "# put all comments in all_text and all labels in all_label\n",
        "all_text = list(dataset['comment'].values)\n",
        "all_label = list(dataset['sentiment'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC4Wv3U_CP6E"
      },
      "source": [
        "# apply level-1 preprocessing on text\n",
        "level_1_processed_text = []\n",
        "p = Preprocessing(level=1)\n",
        "for text in all_text:\n",
        "    level_1_processed_text.append(' '.join(p.preprocess_sentence(text)))\n",
        "\n",
        "# apply level-2 preprocessing on text\n",
        "level_2_processed_text = []\n",
        "p = Preprocessing(level=2)\n",
        "for text in all_text:\n",
        "    level_2_processed_text.append(' '.join(p.preprocess_sentence(text)))\n",
        "\n",
        "# apply level-3 preprocessing on text\n",
        "level_3_processed_text = []\n",
        "p = Preprocessing(level=3)\n",
        "for text in all_text:\n",
        "    level_3_processed_text.append(' '.join(p.preprocess_sentence(text)))\n",
        "\n",
        "# make bow objects of 3 levels of text\n",
        "bow_obj = BagOfWords(min_freq=20)\n",
        "bow_level_1_text = bow_obj.make_vactors(level_1_processed_text)\n",
        "bow_level_2_text = bow_obj.make_vactors(level_2_processed_text)\n",
        "bow_level_3_text = bow_obj.make_vactors(level_3_processed_text)\n",
        "\n",
        "# split data to train-val & test\n",
        "x_train_val_level_1, x_test_level_1, y_train_val_level_1, y_test_level_1 = train_test_split(bow_level_1_text,all_label,test_size=.2,random_state=1)\n",
        "x_train_val_level_2, x_test_level_2, y_train_val_level_2, y_test_level_2 = train_test_split(bow_level_2_text,all_label,test_size=.2,random_state=1)\n",
        "x_train_val_level_3, x_test_level_3, y_train_val_level_3, y_test_level_3 = train_test_split(bow_level_3_text,all_label,test_size=.2,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8hPaieMVh-O"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx1Mc5DpCP6I",
        "outputId": "e6ade809-387a-4308-ee3a-8e911239860b"
      },
      "source": [
        "# apply level-1 \n",
        "lr_clf_level_1 = LogisticRegression()\n",
        "lr_clf_level_1.fit(x_train_val_level_1,y_train_val_level_1)\n",
        "y_test_pred = lr_clf_level_1.predict(x_test_level_1)\n",
        "print('############################ Logistic Regression Level 1 ############################')\n",
        "analysis(y_test_level_1,y_test_pred)\n",
        "\n",
        "# apply level-2\n",
        "lr_clf_level_2 = LogisticRegression()\n",
        "lr_clf_level_2.fit(x_train_val_level_2,y_train_val_level_2)\n",
        "y_test_pred = lr_clf_level_2.predict(x_test_level_2)\n",
        "print('############################ Logistic Regression Level 2 ############################')\n",
        "analysis(y_test_level_2,y_test_pred)\n",
        "\n",
        "# apply level-3\n",
        "lr_clf_level_3 = LogisticRegression()\n",
        "lr_clf_level_3.fit(x_train_val_level_3,y_train_val_level_3)\n",
        "y_test_pred = lr_clf_level_3.predict(x_test_level_3)\n",
        "print('############################ Logistic Regression Level 3 ############################')\n",
        "analysis(y_test_level_3,y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############################ Logistic Regression Level 1 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.90      0.88      0.89      4493\n",
            "    negative       0.89      0.90      0.89      4507\n",
            "\n",
            "    accuracy                           0.89      9000\n",
            "   macro avg       0.89      0.89      0.89      9000\n",
            "weighted avg       0.89      0.89      0.89      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3970  523]\n",
            " [ 450 4057]]\n",
            "Accuracy\n",
            " 0.8918888888888888\n",
            "############################ Logistic Regression Level 2 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.88      0.89      4493\n",
            "    negative       0.89      0.89      0.89      4507\n",
            "\n",
            "    accuracy                           0.89      9000\n",
            "   macro avg       0.89      0.89      0.89      9000\n",
            "weighted avg       0.89      0.89      0.89      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3974  519]\n",
            " [ 487 4020]]\n",
            "Accuracy\n",
            " 0.8882222222222222\n",
            "############################ Logistic Regression Level 3 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.88      0.87      0.87      4493\n",
            "    negative       0.87      0.88      0.88      4507\n",
            "\n",
            "    accuracy                           0.88      9000\n",
            "   macro avg       0.88      0.88      0.88      9000\n",
            "weighted avg       0.88      0.88      0.88      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3925  568]\n",
            " [ 554 3953]]\n",
            "Accuracy\n",
            " 0.8753333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS9CNcMaVpwj"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqLhyu83CP6L",
        "outputId": "be1ca67b-5fcb-47bd-8734-e6f305eb33da"
      },
      "source": [
        "# apply level-1 calssification\n",
        "svm_clf_level_1 = SVC(max_iter=5000)\n",
        "svm_clf_level_1.fit(x_train_val_level_1,y_train_val_level_1)\n",
        "y_test_pred = svm_clf_level_1.predict(x_test_level_1)\n",
        "print('############################ SVM Level 1 ############################')\n",
        "analysis(y_test_level_1,y_test_pred)\n",
        "\n",
        "# apply level-2 calssification\n",
        "svm_clf_level_2 = SVC(max_iter=5000)\n",
        "svm_clf_level_2.fit(x_train_val_level_2,y_train_val_level_2)\n",
        "y_test_pred = svm_clf_level_2.predict(x_test_level_2)\n",
        "print('############################ SVM Level 2 ############################')\n",
        "analysis(y_test_level_2,y_test_pred)\n",
        "\n",
        "# apply level-3 calssification\n",
        "svm_clf_level_3 = SVC(max_iter=5000)\n",
        "svm_clf_level_3.fit(x_train_val_level_3,y_train_val_level_3)\n",
        "y_test_pred = svm_clf_level_3.predict(x_test_level_3)\n",
        "print('############################ SVM Level 3 ############################')\n",
        "analysis(y_test_level_3,y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############################ SVM Level 1 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.87      0.47      0.61      4493\n",
            "    negative       0.64      0.93      0.76      4507\n",
            "\n",
            "    accuracy                           0.70      9000\n",
            "   macro avg       0.75      0.70      0.68      9000\n",
            "weighted avg       0.75      0.70      0.68      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[2122 2371]\n",
            " [ 321 4186]]\n",
            "Accuracy\n",
            " 0.7008888888888889\n",
            "############################ SVM Level 2 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.91      0.60      0.72      4493\n",
            "    negative       0.70      0.94      0.80      4507\n",
            "\n",
            "    accuracy                           0.77      9000\n",
            "   macro avg       0.80      0.77      0.76      9000\n",
            "weighted avg       0.80      0.77      0.76      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[2707 1786]\n",
            " [ 283 4224]]\n",
            "Accuracy\n",
            " 0.7701111111111111\n",
            "############################ SVM Level 3 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.90      0.85      0.88      4493\n",
            "    negative       0.86      0.91      0.88      4507\n",
            "\n",
            "    accuracy                           0.88      9000\n",
            "   macro avg       0.88      0.88      0.88      9000\n",
            "weighted avg       0.88      0.88      0.88      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3832  661]\n",
            " [ 428 4079]]\n",
            "Accuracy\n",
            " 0.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94DeSoVfVscp"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUAQEEtwCP6L",
        "outputId": "b9ac17ff-ce31-4b45-8157-a4831e0db02a"
      },
      "source": [
        "# apply level-1 classification\n",
        "knn_clf_level_1 = KNeighborsClassifier(n_neighbors=9,n_jobs=-1,algorithm='brute')\n",
        "knn_clf_level_1.fit(x_train_val_level_1,y_train_val_level_1)\n",
        "y_test_pred = knn_clf_level_1.predict(x_test_level_1)\n",
        "print('############################ KNN Level 1 ############################')\n",
        "analysis(y_test_level_1,y_test_pred)\n",
        "\n",
        "# apply level-2 classification\n",
        "knn_clf_level_2 = KNeighborsClassifier(n_neighbors=9,n_jobs=-1,algorithm='brute')\n",
        "knn_clf_level_2.fit(x_train_val_level_2,y_train_val_level_2)\n",
        "y_test_pred = knn_clf_level_2.predict(x_test_level_2)\n",
        "print('############################ KNN Level 2 ############################')\n",
        "analysis(y_test_level_2,y_test_pred)\n",
        "\n",
        "# apply level-3 classification\n",
        "knn_clf_level_3 = KNeighborsClassifier(n_neighbors=9,n_jobs=-1,algorithm='brute')\n",
        "knn_clf_level_3.fit(x_train_val_level_3,y_train_val_level_3)\n",
        "y_test_pred = knn_clf_level_3.predict(x_test_level_3)\n",
        "print('############################ KNN Level 3 ############################')\n",
        "analysis(y_test_level_3,y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############################ KNN Level 1 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.67      0.57      0.62      4493\n",
            "    negative       0.63      0.72      0.67      4507\n",
            "\n",
            "    accuracy                           0.65      9000\n",
            "   macro avg       0.65      0.65      0.64      9000\n",
            "weighted avg       0.65      0.65      0.64      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[2565 1928]\n",
            " [1253 3254]]\n",
            "Accuracy\n",
            " 0.6465555555555556\n",
            "############################ KNN Level 2 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.68      0.56      0.61      4493\n",
            "    negative       0.63      0.73      0.68      4507\n",
            "\n",
            "    accuracy                           0.65      9000\n",
            "   macro avg       0.65      0.65      0.65      9000\n",
            "weighted avg       0.65      0.65      0.65      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[2527 1966]\n",
            " [1198 3309]]\n",
            "Accuracy\n",
            " 0.6484444444444445\n",
            "############################ KNN Level 3 ############################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.64      0.60      0.62      4493\n",
            "    negative       0.62      0.67      0.64      4507\n",
            "\n",
            "    accuracy                           0.63      9000\n",
            "   macro avg       0.63      0.63      0.63      9000\n",
            "weighted avg       0.63      0.63      0.63      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[2690 1803]\n",
            " [1505 3002]]\n",
            "Accuracy\n",
            " 0.6324444444444445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7WD7DGNCP6L"
      },
      "source": [
        "#PART B\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFhP5S-yrprU"
      },
      "source": [
        "## In this part i apply previous algorithms using bag of words and word2vec only on 3rd level processing but also i finetune hyperparameters using cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKJpBZN7ZSv1"
      },
      "source": [
        "# this function cross-validate logistic regression and returns best C\n",
        "def cross_validition_logistic_regression(X,Y):\n",
        "  best_score = 0\n",
        "  best_C = None\n",
        "\n",
        "  Cs = np.linspace(.1,20,20)\n",
        "  for C in Cs:\n",
        "    lr_clf = LogisticRegression()\n",
        "    scores = cross_val_score(lr_clf,X,Y,cv=4)\n",
        "    score = np.average(scores)\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_C = C\n",
        "\n",
        "  return best_C\n",
        "\n",
        "# this function cross-validate SVM and returns best C\n",
        "def cross_validition_svm(X,Y):\n",
        "  best_score = 0\n",
        "  best_C = None\n",
        "\n",
        "  Cs = np.linspace(.1,1,3)\n",
        "  for C in Cs:\n",
        "    svm_clf = SVC(max_iter=5000,C=C)\n",
        "    scores = cross_val_score(svm_clf,X,Y,cv=4)\n",
        "    score = np.average(scores)\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_C = C\n",
        "\n",
        "  return best_C\n",
        "\n",
        "# this function cross-validate knn and return best k\n",
        "def cross_validition_knn(X,Y):\n",
        "  best_score = 0\n",
        "  best_k = None\n",
        "\n",
        "  ks = np.arange(3,10)\n",
        "  for k in ks:\n",
        "    knn_clf = KNeighborsClassifier(n_neighbors=k,n_jobs=-1,algorithm='brute')\n",
        "    scores = cross_val_score(knn_clf,X,Y,cv=4)\n",
        "    score = np.average(scores)\n",
        "    if score > best_score:\n",
        "      best_score = score\n",
        "      best_k = k\n",
        "\n",
        "  return best_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svpf_oK9GvyB"
      },
      "source": [
        "## Bag Of Word Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-qcQNCZG0ZR",
        "outputId": "d387545f-6332-42f6-e74b-60a75e3471d0"
      },
      "source": [
        "# apply logistic regressin with cross validation\n",
        "best_C_lr = cross_validition_logistic_regression(x_train_val_level_3,y_train_val_level_3)\n",
        "lr_clf_bow = LogisticRegression(C=best_C_lr)\n",
        "lr_clf_bow.fit(x_train_val_level_3,y_train_val_level_3)\n",
        "y_test_level_3_pred = lr_clf_bow.predict(x_test_level_3)\n",
        "print('################## Logistic Regression (Bag Of Words) ##################')\n",
        "analysis(y_test_level_3,y_test_level_3_pred)\n",
        "lr_clf_bow_accuracy = accuracy_score(y_test_level_3,y_test_level_3_pred)\n",
        "\n",
        "# apply svm with cross validation\n",
        "best_C = cross_validition_svm(x_train_val_level_3,y_train_val_level_3)\n",
        "svm_clf_bow = SVC(max_iter=6000,C=best_C)\n",
        "svm_clf_bow.fit(x_train_val_level_3,y_train_val_level_3)\n",
        "y_test_level_3_pred = svm_clf_bow.predict(x_test_level_3)\n",
        "print('################## SVM (Bag Of Words) ##################')\n",
        "analysis(y_test_level_3,y_test_level_3_pred)\n",
        "svm_clf_bow_accuracy = accuracy_score(y_test_level_3,y_test_level_3_pred)\n",
        "\n",
        "# apply knn with cross validation\n",
        "best_k = cross_validition_knn(x_train_val_level_3,y_train_val_level_3)\n",
        "knn_clf_bow = KNeighborsClassifier(n_neighbors=best_k,n_jobs=-1,algorithm='brute')\n",
        "knn_clf_bow.fit(x_train_val_level_3,y_train_val_level_3)\n",
        "y_test_level_3_pred = knn_clf_bow.predict(x_test_level_3)\n",
        "print('################## KNN (Bag Of Words) ##################')\n",
        "analysis(y_test_level_3,y_test_level_3_pred)\n",
        "knn_clf_bow_accuracy = accuracy_score(y_test_level_3,y_test_level_3_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "################## Logistic Regression (Bag Of Words) ##################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.89      0.89      4493\n",
            "    negative       0.89      0.89      0.89      4507\n",
            "\n",
            "    accuracy                           0.89      9000\n",
            "   macro avg       0.89      0.89      0.89      9000\n",
            "weighted avg       0.89      0.89      0.89      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3979  514]\n",
            " [ 475 4032]]\n",
            "Accuracy\n",
            " 0.8901111111111111\n",
            "################## SVM (Bag Of Words) ##################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.91      0.85      0.88      4493\n",
            "    negative       0.86      0.91      0.89      4507\n",
            "\n",
            "    accuracy                           0.88      9000\n",
            "   macro avg       0.88      0.88      0.88      9000\n",
            "weighted avg       0.88      0.88      0.88      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3838  655]\n",
            " [ 395 4112]]\n",
            "Accuracy\n",
            " 0.8833333333333333\n",
            "################## KNN (Bag Of Words) ##################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.64      0.60      0.62      4493\n",
            "    negative       0.62      0.67      0.64      4507\n",
            "\n",
            "    accuracy                           0.63      9000\n",
            "   macro avg       0.63      0.63      0.63      9000\n",
            "weighted avg       0.63      0.63      0.63      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[2690 1803]\n",
            " [1505 3002]]\n",
            "Accuracy\n",
            " 0.6324444444444445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WegAtL6cZVBr"
      },
      "source": [
        "del level_1_processed_text\n",
        "del level_2_processed_text\n",
        "del level_3_processed_text\n",
        "del bow_level_1_text\n",
        "del bow_level_2_text\n",
        "del bow_level_3_text\n",
        "del x_train_val_level_1\n",
        "del x_train_val_level_2\n",
        "del x_train_val_level_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjaLOlIMG1Er"
      },
      "source": [
        "## Word2Vec Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STAmMSRs_tJz",
        "outputId": "7c54e2ea-435c-49b9-a9e8-b7be0555959a"
      },
      "source": [
        "# download ggole news word vectors and unzip it\n",
        "! wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz \n",
        "! gunzip GoogleNews-vectors-negative300.bin\n",
        "# load word2vec model\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "# preprocess text with level-3 preprocessing and add all text to bew_text array\n",
        "new_text = []\n",
        "p = Preprocessing(level=3)\n",
        "for text in all_text:\n",
        "  current_text = []\n",
        "  processed_text = p.preprocess_sentence(text)\n",
        "  for word in processed_text:\n",
        "    current_text.append(word)\n",
        "  new_text.append(current_text)\n",
        "\n",
        "del all_text\n",
        "\n",
        "# turn text to vectors using loaded word2vec\n",
        "vectorized_text = []\n",
        "for comment in new_text:\n",
        "  temp = []\n",
        "  for word in comment:\n",
        "    try:\n",
        "      temp.append(word2vec_model.wv[word])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  vectorized_text.append(np.average(temp,axis=0))\n",
        "# turn vectorized list to numpy array\n",
        "del new_text\n",
        "del word2vec_model\n",
        "vectorized_text = np.array(vectorized_text)\n",
        "x_train_val_wv, x_test_wv, y_train_val_wv, y_test_wv = train_test_split(vectorized_text,all_label,test_size=.2,random_state=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-07-27 02:50:45--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.137.168\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.137.168|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  81.1MB/s    in 24s     \n",
            "\n",
            "2021-07-27 02:51:09 (65.3 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTWsCtsKbURU",
        "outputId": "47bca108-b242-4769-b5a5-b5d4aef54f3e"
      },
      "source": [
        "# Logistic Regression\n",
        "best_C_lr = cross_validition_logistic_regression(x_train_val_wv,y_train_val_wv)\n",
        "lr_clf_w2v = LogisticRegression(C=best_C_lr)\n",
        "lr_clf_w2v.fit(x_train_val_wv,y_train_val_wv)\n",
        "y_test_wv_pred = lr_clf_w2v.predict(x_test_wv)\n",
        "print('################## Logistic Regression (Word2Vec) ##################')\n",
        "analysis(y_test_wv,y_test_wv_pred)\n",
        "lr_clf_wv_accuracy = accuracy_score(y_test_wv,y_test_wv_pred)\n",
        "\n",
        "# SVM\n",
        "best_C = cross_validition_svm(x_train_val_wv,y_train_val_wv)\n",
        "svm_clf_w2v = SVC(max_iter=6000,C=best_C)\n",
        "svm_clf_w2v.fit(x_train_val_wv,y_train_val_wv)\n",
        "y_test_wv_pred = svm_clf_w2v.predict(x_test_wv)\n",
        "print('################## SVM (Word2Vec) ##################')\n",
        "analysis(y_test_wv,y_test_wv_pred)\n",
        "svm_clf_wv_accuracy = accuracy_score(y_test_wv,y_test_wv_pred)\n",
        "\n",
        "# KNN\n",
        "best_k = cross_validition_knn(x_train_val_wv,y_train_val_wv)\n",
        "knn_clf_w2v = KNeighborsClassifier(n_neighbors=best_k,n_jobs=-1,algorithm='brute')\n",
        "knn_clf_w2v.fit(x_train_val_wv,y_train_val_wv)\n",
        "y_test_wv_pred = knn_clf_w2v.predict(x_test_wv)\n",
        "print('################## KNN (Word2Vec) ##################')\n",
        "analysis(y_test_wv,y_test_wv_pred)\n",
        "knn_clf_wv_accuracy = accuracy_score(y_test_wv,y_test_wv_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "################## Logistic Regression (Word2Vec) ##################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.83      0.83      0.83      4493\n",
            "    negative       0.83      0.83      0.83      4507\n",
            "\n",
            "    accuracy                           0.83      9000\n",
            "   macro avg       0.83      0.83      0.83      9000\n",
            "weighted avg       0.83      0.83      0.83      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3749  744]\n",
            " [ 770 3737]]\n",
            "Accuracy\n",
            " 0.8317777777777777\n",
            "################## SVM (Word2Vec) ##################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.91      0.77      0.83      4493\n",
            "    negative       0.80      0.92      0.86      4507\n",
            "\n",
            "    accuracy                           0.85      9000\n",
            "   macro avg       0.85      0.85      0.85      9000\n",
            "weighted avg       0.85      0.85      0.85      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3463 1030]\n",
            " [ 354 4153]]\n",
            "Accuracy\n",
            " 0.8462222222222222\n",
            "################## KNN (Word2Vec) ##################\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.78      0.81      0.80      4493\n",
            "    negative       0.81      0.77      0.79      4507\n",
            "\n",
            "    accuracy                           0.79      9000\n",
            "   macro avg       0.79      0.79      0.79      9000\n",
            "weighted avg       0.79      0.79      0.79      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3653  840]\n",
            " [1026 3481]]\n",
            "Accuracy\n",
            " 0.7926666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OMCfoDyiji3"
      },
      "source": [
        "## Save best models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VALt0-nMiilL",
        "outputId": "a8b01a47-00c3-44d9-a665-687017f1a8a5"
      },
      "source": [
        "# save all models by their name to drive\n",
        "\n",
        "# load drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# save all models (bow & w2v) to drive\n",
        "pk.dump(lr_clf_w2v, open('/content/drive/My Drive/LR_W2V.pkl', 'wb'))\n",
        "pk.dump(lr_clf_bow, open('/content/drive/My Drive/LR_BOW.pkl', 'wb'))\n",
        "pk.dump(svm_clf_w2v, open('/content/drive/My Drive/SVM_W2V.pkl', 'wb'))\n",
        "pk.dump(svm_clf_bow, open('/content/drive/My Drive/SVM_BOW.pkl', 'wb'))\n",
        "pk.dump(knn_clf_w2v, open('/content/drive/My Drive/KNN_W2V.pkl', 'wb'))\n",
        "pk.dump(knn_clf_bow, open('/content/drive/My Drive/KNN_BOW.pkl', 'wb'))\n",
        "\n",
        "# save best models by name mentioned in pdf to drive\n",
        "if lr_clf_wv_accuracy > lr_clf_bow_accuracy:\n",
        "  pk.dump(lr_clf_w2v, open('/content/drive/My Drive/LR.pkl', 'wb'))\n",
        "else:\n",
        "  pk.dump(lr_clf_bow, open('/content/drive/My Drive/LR.pkl', 'wb'))\n",
        "\n",
        "if svm_clf_wv_accuracy > svm_clf_bow_accuracy:\n",
        "  pk.dump(svm_clf_w2v, open('/content/drive/My Drive/SVM.pkl', 'wb'))\n",
        "else:\n",
        "  pk.dump(svm_clf_bow, open('/content/drive/My Drive/SVM.pkl', 'wb'))\n",
        "\n",
        "if knn_clf_wv_accuracy > knn_clf_bow_accuracy:\n",
        "  pk.dump(knn_clf_w2v, open('/content/drive/My Drive/KNN.pkl', 'wb'))\n",
        "else:\n",
        "  pk.dump(knn_clf_bow, open('/content/drive/My Drive/KNN.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNzMUj-8Jxwt"
      },
      "source": [
        "# PART C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWfS6UCssPrh"
      },
      "source": [
        "## In ths part i apply neural network on dataset to measure the performance l2-coefficient is finetuned using cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVaVmAU-J3OT",
        "outputId": "a5626bf8-e5a0-4ec6-a28e-8fed2b661d14"
      },
      "source": [
        "hidden_size_1 = 100\n",
        "hidden_size_2 = 100\n",
        "alphas = np.linspace(1e-6,1e-3,4)\n",
        "best_score = 0\n",
        "\n",
        "for alpha in alphas:\n",
        "  mlp_clf = MLPClassifier(activation='relu',hidden_layer_sizes=(hidden_size_1,hidden_size_2),solver='adam',learning_rate_init=7e-5,alpha=alpha,learning_rate='adaptive',batch_size=100)\n",
        "  scores = cross_val_score(mlp_clf,x_train_val_wv,y_train_val_wv,cv=4)\n",
        "  score = np.average(scores)\n",
        "  print(f'alpha {alpha} | accuracy on 4-fold cross validation {score}')\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    best_alpha = alpha\n",
        "\n",
        "print('################################')\n",
        "print(f'best alpha is {best_alpha}')\n",
        "print('start training best model on best alpha\\n\\n')\n",
        "\n",
        "best_model = MLPClassifier(activation='relu',hidden_layer_sizes=(hidden_size_1,hidden_size_2),solver='adam',learning_rate_init=7e-5,alpha=best_alpha,batch_size=100,learning_rate='adaptive')\n",
        "best_model.fit(x_train_val_wv,y_train_val_wv)\n",
        "y_test_wv_pred = best_model.predict(x_test_wv)\n",
        "analysis(y_test_wv,y_test_wv_pred)\n",
        "pk.dump(best_model, open('best.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alpha 1e-06 | accuracy on 4-fold cross validation 0.8538333333333333\n",
            "alpha 0.00033400000000000004 | accuracy on 4-fold cross validation 0.8585277777777777\n",
            "alpha 0.0006670000000000001 | accuracy on 4-fold cross validation 0.8610277777777778\n",
            "alpha 0.001 | accuracy on 4-fold cross validation 0.8591944444444445\n",
            "################################\n",
            "best alpha is 0.0006670000000000001\n",
            "start training best model on best alpha\n",
            "\n",
            "\n",
            "Report Classification\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.85      0.88      0.86      4493\n",
            "    negative       0.87      0.84      0.86      4507\n",
            "\n",
            "    accuracy                           0.86      9000\n",
            "   macro avg       0.86      0.86      0.86      9000\n",
            "weighted avg       0.86      0.86      0.86      9000\n",
            "\n",
            "Matrix Confusion\n",
            " [[3932  561]\n",
            " [ 700 3807]]\n",
            "Accuracy\n",
            " 0.8598888888888889\n"
          ]
        }
      ]
    }
  ]
}